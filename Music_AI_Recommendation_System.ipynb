{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsm-wew068/hack-mvp/blob/main/Music_AI_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pehxtcEpnU_P"
      },
      "source": [
        "# üéµ AI Music Recommendation System - Google Colab\n",
        "\n",
        "This notebook runs the complete Music AI Recommendation System with GPT-OSS-20B and neural networks.\n",
        "\n",
        "## üöÄ Features:\n",
        "- AI-powered music recommendations\n",
        "- GPT-OSS-20B natural language explanations\n",
        "- Spotify integration with OAuth\n",
        "- RLHF preference learning\n",
        "- Interactive Streamlit frontend\n",
        "\n",
        "## üìã Setup Instructions:\n",
        "1. **Enable GPU**: Runtime > Change runtime type > GPU (T4 or better)\n",
        "2. **Run all cells** in order\n",
        "3. **Get Spotify API credentials** from https://developer.spotify.com/dashboard\n",
        "4. **Update credentials** in the config cell\n",
        "5. **Enjoy your AI music recommendations!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nboH9-HFnU_Q"
      },
      "source": [
        "## üîß Step 1: Install Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_mnJiTQnU_Q"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q streamlit fastapi uvicorn[standard] pydantic pydantic-settings\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q transformers vllm scikit-learn numpy pandas\n",
        "%pip install -q spotipy sqlalchemy aiosqlite httpx aiohttp requests\n",
        "%pip install -q plotly rich python-dotenv google-colab\n",
        "%pip install -q pyngrok\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El7rD-pVnU_R"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/rsm-wew068/hack-mvp.git\n",
        "%cd hack-mvp\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p models data logs\n",
        "\n",
        "print(\"‚úÖ Repository cloned and directories created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeWCQ1-HnU_R"
      },
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p models data logs\n",
        "\n",
        "print(\"‚úÖ Repository cloned and directories created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKL1Y5ZBnU_R"
      },
      "outputs": [],
      "source": [
        "# Setup ngrok for Colab port forwarding (alternative method)\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Get ngrok authtoken (optional but recommended)\n",
        "print(\"üîó Setting up ngrok for better port forwarding...\")\n",
        "print(\"   You can get a free ngrok authtoken from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "print(\"   (Optional - the app will work without it, but ngrok provides better URLs)\")\n",
        "\n",
        "# Uncomment and add your ngrok authtoken if you have one:\n",
        "# ngrok.set_auth_token(\"your_ngrok_authtoken_here\")\n",
        "\n",
        "print(\"‚úÖ ngrok setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihM941npnU_S"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"üöÄ GPU detected: {gpu_name} (Count: {gpu_count})\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Please enable GPU in Runtime > Change runtime type > GPU\")\n",
        "    print(\"   The system will still work but with slower performance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6N3MGoInU_S"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"üöÄ GPU detected: {gpu_name} (Count: {gpu_count})\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Please enable GPU in Runtime > Change runtime type > GPU\")\n",
        "    print(\"   The system will still work but with slower performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOmdAmNnU_S"
      },
      "source": [
        "## üîê Step 2: Configure Spotify API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Wb-5rsnU_S"
      },
      "outputs": [],
      "source": [
        "# Configure Spotify API credentials\n",
        "import os\n",
        "\n",
        "# Get your credentials from https://developer.spotify.com/dashboard\n",
        "# 1. Create a new app\n",
        "# 2. Add redirect URI: https://colab.research.google.com/callback\n",
        "# 3. Copy Client ID and Client Secret here\n",
        "\n",
        "SPOTIFY_CLIENT_ID = \"your_client_id_here\"  # Replace with your actual Client ID\n",
        "SPOTIFY_CLIENT_SECRET = \"your_client_secret_here\"  # Replace with your actual Client Secret\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"SPOTIFY_CLIENT_ID\"] = SPOTIFY_CLIENT_ID\n",
        "os.environ[\"SPOTIFY_CLIENT_SECRET\"] = SPOTIFY_CLIENT_SECRET\n",
        "os.environ[\"SPOTIFY_REDIRECT_URI\"] = \"https://colab.research.google.com/callback\"\n",
        "\n",
        "# Colab-specific settings\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/hack-mvp\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"VLLM_HOST\"] = \"0.0.0.0\"\n",
        "os.environ[\"VLLM_PORT\"] = \"8002\"\n",
        "os.environ[\"API_BASE_URL\"] = \"http://localhost:8000\"\n",
        "os.environ[\"FRONTEND_URL\"] = \"http://localhost:8501\"\n",
        "os.environ[\"DATABASE_URL\"] = \"sqlite:///./data/music_app.db\"\n",
        "os.environ[\"MODEL_STORAGE_PATH\"] = \"./models\"\n",
        "os.environ[\"DATA_STORAGE_PATH\"] = \"./data\"\n",
        "\n",
        "print(\"‚úÖ Environment configured!\")\n",
        "print(f\"üì± Spotify Client ID: {SPOTIFY_CLIENT_ID[:10]}...\" if SPOTIFY_CLIENT_ID != \"your_client_id_here\" else \"‚ö†Ô∏è  Please update your Spotify credentials above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h007WqQDnU_S"
      },
      "source": [
        "## üß† Step 3: Initialize AI Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzAnKJIbnU_S"
      },
      "outputs": [],
      "source": [
        "# Initialize neural network models\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append('/content/hack-mvp')\n",
        "\n",
        "from models.neural_networks import MusicEmbeddingNet, BradleyTerryModel, DeepCollaborativeFilter\n",
        "\n",
        "print(\"üß† Initializing neural network models...\")\n",
        "\n",
        "# Create models\n",
        "audio_model = MusicEmbeddingNet(num_audio_features=13, embedding_dim=128)\n",
        "bt_model = BradleyTerryModel(num_items=100000, embedding_dim=64)\n",
        "cf_model = DeepCollaborativeFilter(num_users=10000, num_items=100000, embedding_dim=128)\n",
        "\n",
        "# Save models\n",
        "torch.save(audio_model.state_dict(), 'models/audio_embedding.pth')\n",
        "torch.save(bt_model.state_dict(), 'models/bradley_terry.pth')\n",
        "torch.save(cf_model.state_dict(), 'models/collaborative_filter.pth')\n",
        "\n",
        "print(\"‚úÖ Neural network models initialized and saved!\")\n",
        "print(f\"üìä Model sizes:\")\n",
        "print(f\"   Audio Embedding: {sum(p.numel() for p in audio_model.parameters()):,} parameters\")\n",
        "print(f\"   Bradley-Terry: {sum(p.numel() for p in bt_model.parameters()):,} parameters\")\n",
        "print(f\"   Collaborative Filter: {sum(p.numel() for p in cf_model.parameters()):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Cwl4OKnU_S"
      },
      "source": [
        "## üöÄ Step 4: Start All Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qocGKzeynU_S"
      },
      "outputs": [],
      "source": [
        "# Start all services in background\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "from google.colab import output\n",
        "\n",
        "print(\"üöÄ Starting Music AI Recommendation System...\")\n",
        "\n",
        "# Enable Colab port forwarding\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "def start_vllm_server():\n",
        "    \"\"\"Start vLLM server with GPT-OSS-20B.\"\"\"\n",
        "    print(\"üß† Starting vLLM server with GPT-OSS-20B...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.api_server\",\n",
        "        \"--model\", \"openai-community/gpt-oss-20b\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"8002\",\n",
        "        \"--gpu-memory-utilization\", \"0.8\",\n",
        "        \"--max-model-len\", \"4096\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "def start_backend():\n",
        "    \"\"\"Start FastAPI backend.\"\"\"\n",
        "    print(\"‚ö° Starting FastAPI backend...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"uvicorn\",\n",
        "        \"backend.main:app\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"8000\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "def start_frontend():\n",
        "    \"\"\"Start Streamlit frontend.\"\"\"\n",
        "    print(\"üñ•Ô∏è  Starting Streamlit frontend...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"streamlit\", \"run\",\n",
        "        \"frontend/app.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.address\", \"0.0.0.0\",\n",
        "        \"--server.headless\", \"true\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "# Start services in background threads\n",
        "vllm_thread = threading.Thread(target=start_vllm_server, daemon=True)\n",
        "backend_thread = threading.Thread(target=start_backend, daemon=True)\n",
        "frontend_thread = threading.Thread(target=start_frontend, daemon=True)\n",
        "\n",
        "vllm_thread.start()\n",
        "backend_thread.start()\n",
        "frontend_thread.start()\n",
        "\n",
        "print(\"‚è≥ Services starting... This may take a few minutes for the first run.\")\n",
        "print(\"   - vLLM server: Downloading GPT-OSS-20B model (~2-3 minutes)\")\n",
        "print(\"   - Backend: Starting FastAPI server\")\n",
        "print(\"   - Frontend: Starting Streamlit app\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yssuRxGMnU_T"
      },
      "source": [
        "## ‚è≥ Step 5: Wait for Services to Start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV663rpSnU_T"
      },
      "outputs": [],
      "source": [
        "# üîß FIX FOR COLAB CONNECTION ISSUES\n",
        "# If you're getting \"localhost refused to connect\" errors, run this cell\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"üîß Fixing Colab connection issues...\")\n",
        "\n",
        "# Method 1: Use ngrok for better port forwarding\n",
        "try:\n",
        "    print(\"üîó Setting up ngrok tunnels...\")\n",
        "\n",
        "    # Start services first (if not already running)\n",
        "    print(\"üöÄ Starting services...\")\n",
        "\n",
        "    # Start backend\n",
        "    backend_process = subprocess.Popen([\n",
        "        \"python\", \"-m\", \"uvicorn\", \"backend.main:app\",\n",
        "        \"--host\", \"0.0.0.0\", \"--port\", \"8000\"\n",
        "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Start frontend\n",
        "    frontend_process = subprocess.Popen([\n",
        "        \"python\", \"-m\", \"streamlit\", \"run\", \"frontend/app.py\",\n",
        "        \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\", \"--server.headless\", \"true\"\n",
        "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Wait a bit for services to start\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Create ngrok tunnels\n",
        "    frontend_tunnel = ngrok.connect(8501)\n",
        "    backend_tunnel = ngrok.connect(8000)\n",
        "\n",
        "    print(\"‚úÖ Services started with ngrok tunnels!\")\n",
        "    print(f\"üéµ Frontend (Streamlit): {frontend_tunnel.public_url}\")\n",
        "    print(f\"‚ö° Backend (API): {backend_tunnel.public_url}\")\n",
        "    print(f\"üìö API Docs: {backend_tunnel.public_url}/docs\")\n",
        "\n",
        "    # Store URLs for later use\n",
        "    globals()['frontend_url'] = frontend_tunnel.public_url\n",
        "    globals()['backend_url'] = backend_tunnel.public_url\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ngrok setup failed: {e}\")\n",
        "    print(\"üîÑ Trying alternative method...\")\n",
        "\n",
        "    # Method 2: Use Colab's built-in port forwarding\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "    print(\"üì± Using Colab's built-in port forwarding...\")\n",
        "    print(\"   Frontend: http://localhost:8501\")\n",
        "    print(\"   Backend: http://localhost:8000\")\n",
        "    print(\"   API Docs: http://localhost:8000/docs\")\n",
        "\n",
        "    # Try to open the app directly\n",
        "    try:\n",
        "        output.serve_kernel_port_as_window(8501, path='/', anchor_text='üéµ Open Music AI App')\n",
        "        print(\"‚úÖ Click the link above to open the app!\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Could not create direct link. Use the URLs above.\")\n",
        "\n",
        "print(\"\\\\nüéâ Connection fix complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15K1F6uOnU_T"
      },
      "outputs": [],
      "source": [
        "# üö® TROUBLESHOOTING: If you still can't connect\n",
        "\n",
        "print(\"üö® Troubleshooting Connection Issues\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if services are running\n",
        "import subprocess\n",
        "import requests\n",
        "\n",
        "def check_service(port, name):\n",
        "    try:\n",
        "        response = requests.get(f\"http://localhost:{port}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ {name} (port {port}): Running\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {name} (port {port}): Status {response.status_code}\")\n",
        "            return False\n",
        "    except:\n",
        "        print(f\"‚ùå {name} (port {port}): Not responding\")\n",
        "        return False\n",
        "\n",
        "# Check all services\n",
        "print(\"üîç Checking service status:\")\n",
        "backend_ok = check_service(8000, \"Backend API\")\n",
        "frontend_ok = check_service(8501, \"Streamlit Frontend\")\n",
        "\n",
        "if not backend_ok or not frontend_ok:\n",
        "    print(\"\\\\nüîß Services not running. Let's restart them...\")\n",
        "\n",
        "    # Kill any existing processes\n",
        "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"], capture_output=True)\n",
        "    subprocess.run([\"pkill\", \"-f\", \"uvicorn\"], capture_output=True)\n",
        "\n",
        "    print(\"üîÑ Restarting services...\")\n",
        "\n",
        "    # Start backend\n",
        "    subprocess.Popen([\n",
        "        \"python\", \"-m\", \"uvicorn\", \"backend.main:app\",\n",
        "        \"--host\", \"0.0.0.0\", \"--port\", \"8000\"\n",
        "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Start frontend\n",
        "    subprocess.Popen([\n",
        "        \"python\", \"-m\", \"streamlit\", \"run\", \"frontend/app.py\",\n",
        "        \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\", \"--server.headless\", \"true\"\n",
        "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    print(\"‚è≥ Waiting for services to start...\")\n",
        "    time.sleep(15)\n",
        "\n",
        "    # Check again\n",
        "    print(\"\\\\nüîç Rechecking service status:\")\n",
        "    check_service(8000, \"Backend API\")\n",
        "    check_service(8501, \"Streamlit Frontend\")\n",
        "\n",
        "print(\"\\\\nüí° If you still can't connect:\")\n",
        "print(\"   1. Make sure you're using the ngrok URLs (if available)\")\n",
        "print(\"   2. Try refreshing the page\")\n",
        "print(\"   3. Check that your Spotify credentials are set correctly\")\n",
        "print(\"   4. Restart the Colab runtime and run all cells again\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbYlm_a1nU_T"
      },
      "outputs": [],
      "source": [
        "# Wait for services to be ready and test them\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"‚è≥ Waiting for services to start...\")\n",
        "print(\"   This may take 3-5 minutes for the first run (model download)\")\n",
        "\n",
        "# Wait for initial startup\n",
        "time.sleep(30)\n",
        "\n",
        "# Test services\n",
        "def test_service(url, name, timeout=10):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ {name}: Ready (Status: {response.status_code})\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {name}: Status {response.status_code}\")\n",
        "            return False\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚è≥ {name}: Still starting... ({str(e)[:50]}...)\")\n",
        "        return False\n",
        "\n",
        "# Test all services\n",
        "services = [\n",
        "    (\"http://localhost:8000/health\", \"Backend API\"),\n",
        "    (\"http://localhost:8002/health\", \"vLLM Server\"),\n",
        "    (\"http://localhost:8501/_stcore/health\", \"Streamlit Frontend\")\n",
        "]\n",
        "\n",
        "all_ready = False\n",
        "attempts = 0\n",
        "max_attempts = 20\n",
        "\n",
        "while not all_ready and attempts < max_attempts:\n",
        "    attempts += 1\n",
        "    print(f\"\\nüîÑ Attempt {attempts}/{max_attempts}:\")\n",
        "\n",
        "    results = []\n",
        "    for url, name in services:\n",
        "        results.append(test_service(url, name))\n",
        "\n",
        "    all_ready = all(results)\n",
        "\n",
        "    if not all_ready:\n",
        "        print(f\"‚è≥ Waiting 15 seconds before next attempt...\")\n",
        "        time.sleep(15)\n",
        "\n",
        "if all_ready:\n",
        "    print(\"\\nüéâ All services are ready!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some services may still be starting. You can continue or restart the services.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i7iYDY2nU_T"
      },
      "source": [
        "## üéµ Step 6: Access Your Music AI App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPAEY62UnU_T"
      },
      "outputs": [],
      "source": [
        "# Display access information\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "print(\"üéâ Music AI Recommendation System is now running!\")\n",
        "print(\"\\nüì± Access your application:\")\n",
        "print(\"\\nüñ•Ô∏è  Frontend (Streamlit App):\")\n",
        "print(\"   The Streamlit app should be embedded below or accessible through Colab's interface\")\n",
        "\n",
        "# Try to embed the Streamlit app\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_iframe(8501, path='/', anchor_text='üéµ Open Music AI App')\n",
        "    print(\"   ‚úÖ Click the link above to open the app!\")\n",
        "except:\n",
        "    print(\"   üì± App is running on http://localhost:8501\")\n",
        "\n",
        "print(\"\\n‚ö° Backend API:\")\n",
        "print(\"   http://localhost:8000\")\n",
        "print(\"\\nüìö API Documentation:\")\n",
        "print(\"   http://localhost:8000/docs\")\n",
        "\n",
        "print(\"\\nüß† vLLM Server (GPT-OSS-20B):\")\n",
        "print(\"   http://localhost:8002\")\n",
        "\n",
        "print(\"\\nüéØ What you can do:\")\n",
        "print(\"   1. üîê Authenticate with Spotify\")\n",
        "print(\"   2. üéµ Get AI-powered music recommendations\")\n",
        "print(\"   3. üéØ Train the AI with A/B comparisons\")\n",
        "print(\"   4. üë§ View your musical taste profile\")\n",
        "print(\"   5. üìä Explore analytics and insights\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXSBnD9nU_T"
      },
      "source": [
        "## üéÆ Step 7: Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMgw4PO4nU_T"
      },
      "outputs": [],
      "source": [
        "# Run the interactive demo\n",
        "import asyncio\n",
        "import sys\n",
        "sys.path.append('/content/hack-mvp')\n",
        "\n",
        "from demo.showcase import HackathonDemo\n",
        "\n",
        "print(\"üéÆ Running interactive demo...\")\n",
        "print(\"   This showcases all the features of the Music AI system\")\n",
        "\n",
        "demo = HackathonDemo()\n",
        "await demo.run_full_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmF6D_TznU_U"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully set up and running the **AI Music Recommendation System** with:\n",
        "\n",
        "- ‚úÖ **GPT-OSS-20B** for natural language explanations\n",
        "- ‚úÖ **Neural Networks** for personalized recommendations\n",
        "- ‚úÖ **RLHF Training** with interactive A/B comparisons\n",
        "- ‚úÖ **Spotify Integration** with OAuth authentication\n",
        "- ‚úÖ **Real-time Learning** from user feedback\n",
        "- ‚úÖ **Beautiful UI** with analytics and insights\n",
        "\n",
        "### üéØ Next Steps:\n",
        "1. **Authenticate with Spotify** in the app\n",
        "2. **Get AI recommendations** for your playlists\n",
        "3. **Train the AI** using A/B comparisons\n",
        "4. **Explore your musical taste profile**\n",
        "5. **Share your experience** with others!\n",
        "\n",
        "### üöÄ Deploy to Production:\n",
        "- Use the Docker setup for cloud deployment\n",
        "- Scale with multiple GPU instances\n",
        "- Add more AI models and features\n",
        "\n",
        "**Enjoy your AI-powered music discovery! üéµ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}