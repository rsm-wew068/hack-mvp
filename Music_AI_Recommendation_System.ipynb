{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéµ AI Music Recommendation System - Google Colab\n",
        "\n",
        "This notebook runs the complete Music AI Recommendation System with GPT-OSS-20B and neural networks.\n",
        "\n",
        "## üöÄ Features:\n",
        "- AI-powered music recommendations\n",
        "- GPT-OSS-20B natural language explanations\n",
        "- Spotify integration with OAuth\n",
        "- RLHF preference learning\n",
        "- Interactive Streamlit frontend\n",
        "\n",
        "## üìã Setup Instructions:\n",
        "1. **Enable GPU**: Runtime > Change runtime type > GPU (T4 or better)\n",
        "2. **Run all cells** in order\n",
        "3. **Get Spotify API credentials** from https://developer.spotify.com/dashboard\n",
        "4. **Update credentials** in the config cell\n",
        "5. **Enjoy your AI music recommendations!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 1: Install Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q streamlit fastapi uvicorn[standard] pydantic pydantic-settings\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q transformers vllm scikit-learn numpy pandas\n",
        "%pip install -q spotipy sqlalchemy aiosqlite httpx aiohttp requests\n",
        "%pip install -q plotly rich python-dotenv google-colab\n",
        "%pip install -q pyngrok\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/rsm-wew068/hack-mvp.git\n",
        "%cd hack-mvp\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p models data logs\n",
        "\n",
        "print(\"‚úÖ Repository cloned and directories created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p models data logs\n",
        "\n",
        "print(\"‚úÖ Repository cloned and directories created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok for Colab port forwarding (alternative method)\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Get ngrok authtoken (optional but recommended)\n",
        "print(\"üîó Setting up ngrok for better port forwarding...\")\n",
        "print(\"   You can get a free ngrok authtoken from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "print(\"   (Optional - the app will work without it, but ngrok provides better URLs)\")\n",
        "\n",
        "# Uncomment and add your ngrok authtoken if you have one:\n",
        "# ngrok.set_auth_token(\"your_ngrok_authtoken_here\")\n",
        "\n",
        "print(\"‚úÖ ngrok setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"üöÄ GPU detected: {gpu_name} (Count: {gpu_count})\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Please enable GPU in Runtime > Change runtime type > GPU\")\n",
        "    print(\"   The system will still work but with slower performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Step 2: Configure Spotify API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Spotify API credentials\n",
        "import os\n",
        "\n",
        "# Get your credentials from https://developer.spotify.com/dashboard\n",
        "# 1. Create a new app\n",
        "# 2. Add redirect URI: https://colab.research.google.com/callback\n",
        "# 3. Copy Client ID and Client Secret here\n",
        "\n",
        "SPOTIFY_CLIENT_ID = \"your_client_id_here\"  # Replace with your actual Client ID\n",
        "SPOTIFY_CLIENT_SECRET = \"your_client_secret_here\"  # Replace with your actual Client Secret\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"SPOTIFY_CLIENT_ID\"] = SPOTIFY_CLIENT_ID\n",
        "os.environ[\"SPOTIFY_CLIENT_SECRET\"] = SPOTIFY_CLIENT_SECRET\n",
        "os.environ[\"SPOTIFY_REDIRECT_URI\"] = \"https://colab.research.google.com/callback\"\n",
        "\n",
        "# Colab-specific settings\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/hack-mvp\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"VLLM_HOST\"] = \"0.0.0.0\"\n",
        "os.environ[\"VLLM_PORT\"] = \"8002\"\n",
        "os.environ[\"API_BASE_URL\"] = \"http://localhost:8000\"\n",
        "os.environ[\"FRONTEND_URL\"] = \"http://localhost:8501\"\n",
        "os.environ[\"DATABASE_URL\"] = \"sqlite:///./data/music_app.db\"\n",
        "os.environ[\"MODEL_STORAGE_PATH\"] = \"./models\"\n",
        "os.environ[\"DATA_STORAGE_PATH\"] = \"./data\"\n",
        "\n",
        "print(\"‚úÖ Environment configured!\")\n",
        "print(f\"üì± Spotify Client ID: {SPOTIFY_CLIENT_ID[:10]}...\" if SPOTIFY_CLIENT_ID != \"your_client_id_here\" else \"‚ö†Ô∏è  Please update your Spotify credentials above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Step 3: Initialize AI Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize neural network models\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append('/content/hack-mvp')\n",
        "\n",
        "from models.neural_networks import MusicEmbeddingNet, BradleyTerryModel, DeepCollaborativeFilter\n",
        "\n",
        "print(\"üß† Initializing neural network models...\")\n",
        "\n",
        "# Create models\n",
        "audio_model = MusicEmbeddingNet(num_audio_features=13, embedding_dim=128)\n",
        "bt_model = BradleyTerryModel(num_items=100000, embedding_dim=64)\n",
        "cf_model = DeepCollaborativeFilter(num_users=10000, num_items=100000, embedding_dim=128)\n",
        "\n",
        "# Save models\n",
        "torch.save(audio_model.state_dict(), 'models/audio_embedding.pth')\n",
        "torch.save(bt_model.state_dict(), 'models/bradley_terry.pth')\n",
        "torch.save(cf_model.state_dict(), 'models/collaborative_filter.pth')\n",
        "\n",
        "print(\"‚úÖ Neural network models initialized and saved!\")\n",
        "print(f\"üìä Model sizes:\")\n",
        "print(f\"   Audio Embedding: {sum(p.numel() for p in audio_model.parameters()):,} parameters\")\n",
        "print(f\"   Bradley-Terry: {sum(p.numel() for p in bt_model.parameters()):,} parameters\")\n",
        "print(f\"   Collaborative Filter: {sum(p.numel() for p in cf_model.parameters()):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 4: Start All Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start all services in background\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "from google.colab import output\n",
        "\n",
        "print(\"üöÄ Starting Music AI Recommendation System...\")\n",
        "\n",
        "# Enable Colab port forwarding\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "def start_vllm_server():\n",
        "    \"\"\"Start vLLM server with GPT-OSS-20B.\"\"\"\n",
        "    print(\"üß† Starting vLLM server with GPT-OSS-20B...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.api_server\",\n",
        "        \"--model\", \"openai-community/gpt-oss-20b\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"8002\",\n",
        "        \"--gpu-memory-utilization\", \"0.8\",\n",
        "        \"--max-model-len\", \"4096\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "def start_backend():\n",
        "    \"\"\"Start FastAPI backend.\"\"\"\n",
        "    print(\"‚ö° Starting FastAPI backend...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"uvicorn\",\n",
        "        \"backend.main:app\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"8000\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "def start_frontend():\n",
        "    \"\"\"Start Streamlit frontend.\"\"\"\n",
        "    print(\"üñ•Ô∏è  Starting Streamlit frontend...\")\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"streamlit\", \"run\",\n",
        "        \"frontend/app.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.address\", \"0.0.0.0\",\n",
        "        \"--server.headless\", \"true\"\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "\n",
        "# Start services in background threads\n",
        "vllm_thread = threading.Thread(target=start_vllm_server, daemon=True)\n",
        "backend_thread = threading.Thread(target=start_backend, daemon=True)\n",
        "frontend_thread = threading.Thread(target=start_frontend, daemon=True)\n",
        "\n",
        "vllm_thread.start()\n",
        "backend_thread.start()\n",
        "frontend_thread.start()\n",
        "\n",
        "print(\"‚è≥ Services starting... This may take a few minutes for the first run.\")\n",
        "print(\"   - vLLM server: Downloading GPT-OSS-20B model (~2-3 minutes)\")\n",
        "print(\"   - Backend: Starting FastAPI server\")\n",
        "print(\"   - Frontend: Starting Streamlit app\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è≥ Step 5: Wait for Services to Start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for services to be ready and test them\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"‚è≥ Waiting for services to start...\")\n",
        "print(\"   This may take 3-5 minutes for the first run (model download)\")\n",
        "\n",
        "# Wait for initial startup\n",
        "time.sleep(30)\n",
        "\n",
        "# Test services\n",
        "def test_service(url, name, timeout=10):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ {name}: Ready (Status: {response.status_code})\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {name}: Status {response.status_code}\")\n",
        "            return False\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚è≥ {name}: Still starting... ({str(e)[:50]}...)\")\n",
        "        return False\n",
        "\n",
        "# Test all services\n",
        "services = [\n",
        "    (\"http://localhost:8000/health\", \"Backend API\"),\n",
        "    (\"http://localhost:8002/health\", \"vLLM Server\"),\n",
        "    (\"http://localhost:8501/_stcore/health\", \"Streamlit Frontend\")\n",
        "]\n",
        "\n",
        "all_ready = False\n",
        "attempts = 0\n",
        "max_attempts = 20\n",
        "\n",
        "while not all_ready and attempts < max_attempts:\n",
        "    attempts += 1\n",
        "    print(f\"\\nüîÑ Attempt {attempts}/{max_attempts}:\")\n",
        "    \n",
        "    results = []\n",
        "    for url, name in services:\n",
        "        results.append(test_service(url, name))\n",
        "    \n",
        "    all_ready = all(results)\n",
        "    \n",
        "    if not all_ready:\n",
        "        print(f\"‚è≥ Waiting 15 seconds before next attempt...\")\n",
        "        time.sleep(15)\n",
        "\n",
        "if all_ready:\n",
        "    print(\"\\nüéâ All services are ready!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some services may still be starting. You can continue or restart the services.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéµ Step 6: Access Your Music AI App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display access information\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "print(\"üéâ Music AI Recommendation System is now running!\")\n",
        "print(\"\\nüì± Access your application:\")\n",
        "print(\"\\nüñ•Ô∏è  Frontend (Streamlit App):\")\n",
        "print(\"   The Streamlit app should be embedded below or accessible through Colab's interface\")\n",
        "\n",
        "# Try to embed the Streamlit app\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_iframe(8501, path='/', anchor_text='üéµ Open Music AI App')\n",
        "    print(\"   ‚úÖ Click the link above to open the app!\")\n",
        "except:\n",
        "    print(\"   üì± App is running on http://localhost:8501\")\n",
        "\n",
        "print(\"\\n‚ö° Backend API:\")\n",
        "print(\"   http://localhost:8000\")\n",
        "print(\"\\nüìö API Documentation:\")\n",
        "print(\"   http://localhost:8000/docs\")\n",
        "\n",
        "print(\"\\nüß† vLLM Server (GPT-OSS-20B):\")\n",
        "print(\"   http://localhost:8002\")\n",
        "\n",
        "print(\"\\nüéØ What you can do:\")\n",
        "print(\"   1. üîê Authenticate with Spotify\")\n",
        "print(\"   2. üéµ Get AI-powered music recommendations\")\n",
        "print(\"   3. üéØ Train the AI with A/B comparisons\")\n",
        "print(\"   4. üë§ View your musical taste profile\")\n",
        "print(\"   5. üìä Explore analytics and insights\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Step 7: Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the interactive demo\n",
        "import asyncio\n",
        "import sys\n",
        "sys.path.append('/content/hack-mvp')\n",
        "\n",
        "from demo.showcase import HackathonDemo\n",
        "\n",
        "print(\"üéÆ Running interactive demo...\")\n",
        "print(\"   This showcases all the features of the Music AI system\")\n",
        "\n",
        "demo = HackathonDemo()\n",
        "await demo.run_full_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully set up and running the **AI Music Recommendation System** with:\n",
        "\n",
        "- ‚úÖ **GPT-OSS-20B** for natural language explanations\n",
        "- ‚úÖ **Neural Networks** for personalized recommendations\n",
        "- ‚úÖ **RLHF Training** with interactive A/B comparisons\n",
        "- ‚úÖ **Spotify Integration** with OAuth authentication\n",
        "- ‚úÖ **Real-time Learning** from user feedback\n",
        "- ‚úÖ **Beautiful UI** with analytics and insights\n",
        "\n",
        "### üéØ Next Steps:\n",
        "1. **Authenticate with Spotify** in the app\n",
        "2. **Get AI recommendations** for your playlists\n",
        "3. **Train the AI** using A/B comparisons\n",
        "4. **Explore your musical taste profile**\n",
        "5. **Share your experience** with others!\n",
        "\n",
        "### üöÄ Deploy to Production:\n",
        "- Use the Docker setup for cloud deployment\n",
        "- Scale with multiple GPU instances\n",
        "- Add more AI models and features\n",
        "\n",
        "**Enjoy your AI-powered music discovery! üéµ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
